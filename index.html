<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding the Cost Function and Gradients</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #ffffff;
            font-size: 150%;
        }
        section {
            margin-bottom: 20px;
            padding: 20px;
            background-color: #ffffff;
            display: none;
            opacity: 0;
            transition: opacity 0.5s ease-in;
        }
        h1, h2, h3, h4 {
            color: #333;
            margin-top: 20px;
        }
        p, li {
            line-height: 1.6;
            color: #444;
            margin-bottom: 20px;
        }
        ul {
            padding-left: 20px;
        }
        .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            text-align: left;
        }
        .image-placeholder img, .interactive-placeholder img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
        }
        .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
        }
        .vocab-section {
            background-color: #f0f8ff;
        }
        .vocab-section h3 {
            color: #1e90ff;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .vocab-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .vocab-term {
            font-weight: bold;
            color: #1e90ff;
        }
        .why-it-matters {
            background-color: #ffe6f0;
        }
        .why-it-matters h3 {
            color: #d81b60;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .stop-and-think {
            background-color: #e6e6ff;
        }
        .stop-and-think h3 {
            color: #4b0082;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .continue-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #007bff;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .reveal-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #4b0082;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .test-your-knowledge {
            background-color: #e6ffe6;
        }
        .test-your-knowledge h3 {
            color: #28a745;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .test-your-knowledge h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .test-your-knowledge p {
            margin-bottom: 15px;
        }
        .check-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #28a745;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
            border: none;
            font-size: 1em;
        }
        .faq-section {
            background-color: #fffbea;
        }
        .faq-section h3 {
            color: #ffcc00;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .faq-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <section id="section1">
        <h1>Understanding the Cost Function and Gradients</h1>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Image showing a magnifying glass over mathematical equations, specifically focusing on derivative symbols, symbolizing the deeper dive into mathematical details of gradient descent.">
        </div>
        <p>Welcome back! 👋 In our first lesson, we conquered the high-level idea of Gradient Descent – the smart hiker going downhill. Now, it's time to zoom in and understand two crucial elements that make Gradient Descent work: the <strong>Cost Function</strong> and <strong>Gradients</strong>. Think of these as the map and compass for our hiker. In this lesson, we'll get a bit more mathematical, but don't worry, we'll take it step by step. Let's get started!</p>
        <div class="continue-button" onclick="showNextSection(2)">Continue</div>
    </section>

    <section id="section2">
        <h2>The Cost Function: Measuring Error</h2>
        <p>Remember, the <strong>cost function</strong> is what we want to minimize. It tells us 'how bad' our model's predictions are. For today, let's focus on a common type of Machine Learning problem: <strong>Linear Regression</strong>.</p>
        <div class="continue-button" onclick="showNextSection(3)">Continue</div>
    </section>

    <section id="section3">
        <p>In Linear Regression, we want to find a line that best fits our data points. Imagine you have data points scattered on a graph, and you want to draw a straight line through them that's as close as possible to all the points.</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Image showing a scatter plot of data points with a straight line drawn through them, but not perfectly fitting all points, illustrating the concept of linear regression and error.">
        </div>
        <div class="continue-button" onclick="showNextSection(4)">Continue</div>
    </section>

    <section id="section4">
        <p>To measure 'how close' the line is, we use the <strong>Mean Squared Error (MSE)</strong> cost function. MSE calculates the average of the squared differences between the actual values (our data points) and the values predicted by our line.</p>
        <div class="continue-button" onclick="showNextSection(5)">Continue</div>
    </section>

    <section id="section5">
        <p>Let's say we have $$n$$ data points, and for each point $$i$$, the actual value is $$y^{(i)}$$, and our linear model predicts $$f_{\beta}(x^{(i)})$$. Here, $$x^{(i)}$$ represents the input features, and $$\beta$$ represents the parameters of our linear model (like the slope and intercept of the line).</p>
        <div class="continue-button" onclick="showNextSection(6)">Continue</div>
    </section>

    <section id="section6">
        <p>The <strong>Mean Squared Error (MSE)</strong> cost function, often denoted as $$C(\beta)$$, is defined as:</p>
        <p>\[C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - f_{\beta}(x^{(i)}))^2\]</p>
        <p>Let's break this down:</p>
        <div class="continue-button" onclick="showNextSection(7)">Continue</div>
    </section>

    <section id="section7">
        <ul>
            <li>$$\sum_{i=1}^{n}$$: This is the summation symbol, meaning we sum up the following term for all data points from $$i=1$$ to $$n$$.</li>
            <li>$$(y^{(i)} - f_{\beta}(x^{(i)}))^2$$: This is the <strong>squared error</strong> for the $$i$$-th data point. It's the difference between the actual value $$y^{(i)}$$ and our model's prediction $$f_{\beta}(x^{(i)})$$, squared. We square it to ensure errors are always positive and to penalize larger errors more heavily.</li>
            <li>$$\frac{1}{2n}$$: We divide by $$2n$$ to average the squared errors over all data points. The factor of 2 is often included for mathematical convenience when we calculate gradients later.</li>
        </ul>
        <p>So, the MSE cost function gives us a single number that represents the overall 'mismatch' between our linear model and the actual data. Our goal in Gradient Descent is to adjust the parameters $$\beta$$ of our linear model to <em>minimize</em> this MSE cost function.</p>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Mean Squared Error (MSE)</h4>
            <p>A cost function that measures the average of the squared differences between predicted and actual values. Commonly used in regression problems.</p>
        </div>
        <p>The lower the MSE, the better our linear model fits the data!</p>
        <div class="continue-button" onclick="showNextSection(8)">Continue</div>
    </section>

    <section id="section8">
        <h2>Gradients: Finding the Downhill Direction</h2>
        <p>Now, how do we find the 'downhill' direction of this MSE cost function? That's where <strong>gradients</strong> come in! Remember from Lesson 1, the gradient $$\nabla_{\beta}C(\beta)$$ points uphill. We need the <em>negative</em> gradient to go downhill.</p>
        <div class="continue-button" onclick="showNextSection(9)">Continue</div>
    </section>

    <section id="section9">
        <p>For our linear regression model, let's assume it's a simple one with two parameters: $$\beta_0$$ (the intercept) and $$\beta_1$$ (the slope for a single feature $$x$$). So, our model is: $$f_{\beta}(x) = \beta_0 + \beta_1x$$.</p>
        <div class="continue-button" onclick="showNextSection(10)">Continue</div>
    </section>

    <section id="section10">
        <p>To use Gradient Descent, we need to calculate the <strong>partial derivatives</strong> of the MSE cost function $$C(\beta)$$ with respect to each parameter: $$\beta_0$$ and $$\beta_1$$. These partial derivatives are the components of our gradient.</p>
        <div class="continue-button" onclick="showNextSection(11)">Continue</div>
    </section>

    <section id="section11">
        <p>Let's calculate the partial derivative with respect to $$\beta_0$$, denoted as $$\frac{\partial C}{\partial \beta_0}$$:</p>
        <p>\[\frac{\partial C}{\partial \beta_0} = \frac{\partial}{\partial \beta_0} \left[ \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - (\beta_0 + \beta_1x^{(i)}))^2 \right]\]</p>
        <p>Using the chain rule of calculus, we get:</p>
        <div class="continue-button" onclick="showNextSection(12)">Continue</div>
    </section>

    <section id="section12">
        <p>\[\frac{\partial C}{\partial \beta_0} = \frac{1}{2n} \sum_{i=1}^{n} 2(y^{(i)} - (\beta_0 + \beta_1x^{(i)})) \cdot (-1)\]</p>
        <p>Simplifying, we get:</p>
        <div class="continue-button" onclick="showNextSection(13)">Continue</div>
    </section>

    <section id="section13">
        <p>\[\frac{\partial C}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} ((\beta_0 + \beta_1x^{(i)}) - y^{(i)}) = \frac{1}{n} \sum_{i=1}^{n} (f_{\beta}(x^{(i)}) - y^{(i)})\]</p>
        <p>So, the gradient with respect to $$\beta_0$$ is the average of the prediction errors.</p>
        <div class="continue-button" onclick="showNextSection(14)">Continue</div>
    </section>

    <section id="section14">
        <p>Similarly, let's calculate the partial derivative with respect to $$\beta_1$$, denoted as $$\frac{\partial C}{\partial \beta_1}$$:</p>
        <p>\[\frac{\partial C}{\partial \beta_1} = \frac{\partial}{\partial \beta_1} \left[ \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - (\beta_0 + \beta_1x^{(i)}))^2 \right]\]</p>
        <p>Using the chain rule again, we get:</p>
        <div class="continue-button" onclick="showNextSection(15)">Continue</div>
    </section>

    <section id="section15">
        <p>\[\frac{\partial C}{\partial \beta_1} = \frac{1}{2n} \sum_{i=1}^{n} 2(y^{(i)} - (\beta_0 + \beta_1x^{(i)})) \cdot (-x^{(i)})\]</p>
        <p>Simplifying, we get:</p>
        <div class="continue-button" onclick="showNextSection(16)">Continue</div>
    </section>

    <section id="section16">
        <p>\[\frac{\partial C}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} x^{(i)} ((\beta_0 + \beta_1x^{(i)}) - y^{(i)}) = \frac{1}{n} \sum_{i=1}^{n} x^{(i)} (f_{\beta}(x^{(i)}) - y^{(i)})\]</p>
        <p>So, the gradient with respect to $$\beta_1$$ is the average of the prediction errors multiplied by the input features $$x^{(i)}$$.</p>
        <div class="continue-button" onclick="showNextSection(17)">Continue</div>
    </section>

    <section id="section17">
        <p>Now we have both components of our gradient: $$\nabla_{\beta}C(\beta) = \begin{bmatrix} \frac{\partial C}{\partial \beta_0} \\ \frac{\partial C}{\partial \beta_1} \end{bmatrix}$$. We can use these gradients in our Gradient Descent update rule!</p>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Animated derivation steps for ∂C/∂β₀ and ∂C/∂β₁. Show each step of the chain rule and simplification process with clear annotations and color-coding to highlight the terms being derived.">
        </div>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Gradient ($$\nabla_{\beta}C(\beta)$$)</h4>
            <p>A vector of partial derivatives of the cost function with respect to each parameter. It indicates the direction of steepest ascent.</p>
        </div>
        <p>These gradients are our 'compass' for stepping downhill!</p>
        <div class="continue-button" onclick="showNextSection(18)">Continue</div>
    </section>

    <section id="section18">
        <h2>Example Gradient Calculation</h2>
        <p>Let's make this concrete with a tiny example. Suppose we have just one data point: $$x^{(1)} = 2$$, $$y^{(1)} = 5$$. And let's say our current parameters are $$\beta_0 = 1$$, $$\beta_1 = 1$$.</p>
        <div class="continue-button" onclick="showNextSection(19)">Continue</div>
    </section>

    <section id="section19">
        <p>Our linear model prediction is: $$f_{\beta}(x^{(1)}) = \beta_0 + \beta_1x^{(1)} = 1 + 1 \times 2 = 3$$.</p>
        <div class="continue-button" onclick="showNextSection(20)">Continue</div>
    </section>

    <section id="section20">
        <p>The prediction error is: $$f_{\beta}(x^{(1)}) - y^{(1)} = 3 - 5 = -2$$.</p>
        <div class="continue-button" onclick="showNextSection(21)">Continue</div>
    </section>

    <section id="section21">
        <p>Now, let's calculate the gradients:</p>
        <div class="continue-button" onclick="showNextSection(22)">Continue</div>
    </section>

    <section id="section22">
        <p>\[\frac{\partial C}{\partial \beta_0} = \frac{1}{1} \sum_{i=1}^{1} (f_{\beta}(x^{(i)}) - y^{(i)}) = (3 - 5) = -2\]</p>
        <div class="continue-button" onclick="showNextSection(23)">Continue</div>
    </section>

    <section id="section23">
        <p>\[\frac{\partial C}{\partial \beta_1} = \frac{1}{1} \sum_{i=1}^{1} x^{(i)} (f_{\beta}(x^{(i)}) - y^{(i)}) = 2 \times (3 - 5) = -4\]</p>
        <div class="continue-button" onclick="showNextSection(24)">Continue</div>
    </section>

    <section id="section24">
        <p>So, our gradient is $$\nabla_{\beta}C(\beta) = \begin{bmatrix} -2 \\ -4 \end{bmatrix}$$. This tells us that to <em>increase</em> the cost function the most, we should increase $$\beta_0$$ and $$\beta_1$$ in the direction of $$[-2, -4]$$. To <em>decrease</em> the cost function (which is our goal), we should move in the <em>opposite</em> direction!</p>
        <div class="interactive-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Interactive Gradient Calculator. Input fields for x^(1), y^(1), β₀, β₁. A 'Calculate Gradients' button. Output fields to display calculated ∂C/∂β₀ and ∂C/∂β₁. Allow students to change input values and recalculate gradients to see how they change dynamically.">
        </div>
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Partial Derivative ($$\frac{\partial C}{\partial \beta_j}$$)</h4>
            <p>The derivative of a multivariable function with respect to one variable, keeping others constant. In Gradient Descent, we use partial derivatives of the cost function with respect to each parameter.</p>
        </div>
        <p>Understanding gradients is key to making Gradient Descent work!</p>
        <div class="continue-button" onclick="showNextSection(25)">Continue</div>
    </section>

    <section id="section25">
        <h2>Why Squared Error?</h2>
        <div class="faq-section">
            <h3>Frequently Asked Questions</h3>
            <h4>Why do we use squared error in MSE instead of just the absolute error $$|y - \hat{y}|$$?</h4>
            <p>Using squared error has several advantages. Mathematically, it makes the cost function differentiable everywhere, which is essential for Gradient Descent to work smoothly. The square function also penalizes larger errors more significantly than smaller errors, which is often desirable. Furthermore, it results in a convex cost function for linear regression, guaranteeing that Gradient Descent can find the global minimum.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(26)">Continue</div>
    </section>

    <section id="section26">
        <h2>Test Your Knowledge</h2>
        <div class="test-your-knowledge">
            <h3>Test Your Knowledge</h3>
            <h4>For a linear regression problem, you've calculated the gradient of the MSE cost function with respect to $$\beta_1$$ and found it to be -3. If you are using Gradient Descent with a learning rate of $$\alpha = 0.1$$, how should you update $$\beta_1$$?</h4>
            <button class="reveal-button" onclick="revealAnswer('test-your-knowledge-1')">Reveal Answer</button>
            <div id="test-your-knowledge-1" style="display: none;">
                <p><strong>Correct Answer:</strong> Increase $$\beta_1$$ by 0.3</p>
                <p><strong>Explanation:</strong> The update rule is $$\beta_{1,new} = \beta_{1,old} - \alpha \frac{\partial C}{\partial \beta_1} = \beta_{1,old} - 0.1 \times (-3) = \beta_{1,old} + 0.3$$. So, you increase $$\beta_1$$ by 0.3.</p>
            </div>
        </div>
        <div class="continue-button" onclick="showNextSection(27)">Continue</div>
    </section>

    <section id="section27">
        <h2>Wrapping Up</h2>
        <p>Excellent work! 🎉 You've now taken a significant step by understanding the cost function and how to calculate gradients for linear regression. These are the essential tools for making Gradient Descent work in practice. In our next lesson, we'll put it all together and see the complete Gradient Descent algorithm in action. Get ready to implement your own optimization process!</p>
    </section>

    <script>
        // Show the first section initially
        document.getElementById("section1").style.display = "block";
        document.getElementById("section1").style.opacity = "1";

        function showNextSection(nextSectionId) {
            const currentButton = event.target;
            const nextSection = document.getElementById("section" + nextSectionId);
            
            currentButton.style.display = "none";
            
            nextSection.style.display = "block";
            setTimeout(() => {
                nextSection.style.opacity = "1";
            }, 10);

            setTimeout(() => {
                nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 500);
        }

        function revealAnswer(id) {
            const revealText = document.getElementById(id);
            const revealButton = event.target;
            
            revealText.style.display = "block";
            revealButton.style.display = "none";
        }
    </script>
</body>
</html>
