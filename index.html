<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Descent for Linear Regression</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #ffffff;
            font-size: 150%;
        }
        section {
            margin-bottom: 20px;
            padding: 20px;
            background-color: #ffffff;
            display: none;
            opacity: 0;
            transition: opacity 0.5s ease-in;
        }
        h1, h2, h3, h4 {
            color: #333;
            margin-top: 20px;
        }
        p, li {
            line-height: 1.6;
            color: #444;
            margin-bottom: 20px;
        }
        ul {
            padding-left: 20px;
        }
        .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            text-align: left;
        }
        .image-placeholder img, .interactive-placeholder img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
        }
        .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
        }
        .vocab-section {
            background-color: #f0f8ff;
        }
        .vocab-section h3 {
            color: #1e90ff;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .vocab-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .vocab-term {
            font-weight: bold;
            color: #1e90ff;
        }
        .why-it-matters {
            background-color: #ffe6f0;
        }
        .why-it-matters h3 {
            color: #d81b60;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .stop-and-think {
            background-color: #e6e6ff;
        }
        .stop-and-think h3 {
            color: #4b0082;
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .continue-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #007bff;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .reveal-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #4b0082;
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
        }
        .test-your-knowledge {
            background-color: #e6ffe6; /* Light green background */
        }
        .test-your-knowledge h3 {
            color: #28a745; /* Dark green heading */
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .test-your-knowledge h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .test-your-knowledge p {
            margin-bottom: 15px;
        }
        .check-button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 15px;
            color: #ffffff;
            background-color: #28a745; /* Green background */
            border-radius: 5px;
            text-decoration: none;
            cursor: pointer;
            border: none;
            font-size: 1em;
        }

        .faq-section {
            background-color: #fffbea; /* Light yellow background */
        }
        .faq-section h3 {
            color: #ffcc00; /* Bright yellow heading */
            font-size: 0.75em;
            margin-bottom: 5px;
            margin-top: 5px;
        }
        .faq-section h4 {
            color: #000;
            font-size: 0.9em;
            margin-top: 10px;
            margin-bottom: 8px;
        }
        .math-explainer {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        .math-step {
            margin-bottom: 15px;
        }
        .math-step-explanation {
            font-style: italic;
            color: #555;
            margin-top: 5px;
        }
        .options-container {
            margin-top: 15px;
        }
        .option {
            padding: 10px;
            margin-bottom: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
            cursor: pointer;
        }
        .option:hover {
            background-color: #f5f5f5;
        }
        .option-explanation {
            margin-top: 10px;
            padding: 10px;
            background-color: #f0f8ff;
            border-radius: 5px;
            display: none;
        }
        .correct {
            border-color: #28a745;
            background-color: #d4edda;
        }
        .incorrect {
            border-color: #dc3545;
            background-color: #f8d7da;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <section id="section1">
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Animated GIF: A line trying to fit a scatter plot of points, iteratively adjusting its slope and intercept.">
        </div>
        <h1>Lesson 2: Gradient Descent for Linear Regression - Calculating the Steps</h1>
        <p>In the last lesson, we learned the general recipe for Gradient Descent: <code>new parameters = old parameters - learning rate * gradient</code>. It's like having instructions to walk downhill, but we need to know <em>which way</em> downhill is!</p>
        <p>That crucial direction comes from the <strong>gradient</strong> (\(\nabla C\)). But how do we actually calculate this gradient? The answer depends on our specific model and how we measure its error (our cost function).</p>
        <p>Let's get concrete and apply Gradient Descent to one of the most fundamental models: <strong>Linear Regression</strong>.</p>
        <div class="continue-button" onclick="showNextSection(2)">Continue</div>
    </section>

    <section id="section2">
        <h2>Linear Regression & Measuring Error (Cost)</h2>
        <p>Remember linear regression? Our goal is to fit a line (or a plane, or hyperplane in higher dimensions) to our data. The equation for our model's prediction, \(f_{\beta}(x^{(i)})\), for a data point \(x^{(i)}\) with \(m\) features is:</p>
         <div class="continue-button" onclick="showNextSection(3)">Continue</div>
    </section>

    <section id="section3">
      
                <p>\[ f_{\beta}(x^{(i)}) = \beta_0 + \beta_1 x_1^{(i)} + \beta_2 x_2^{(i)} + \dots + \beta_m x_m^{(i)} \]</p>
                <p class="math-step-explanation">Here, \(x_1^{(i)}, x_2^{(i)}, \dots, x_m^{(i)}\) are the feature values for the \(i\)-th data point. \(\beta_0\) is the intercept (or bias), and \(\beta_1, \dots, \beta_m\) are the coefficients (or weights) for each feature. These \(\beta\)'s are the parameters we want Gradient Descent to learn!</p>
       
        
        <p>Now, how do we measure how 'wrong' our line is? The most common way is using the <strong>Mean Squared Error (MSE)</strong> cost function, \(C(\beta)\).</p>
        
        <div class="continue-button" onclick="showNextSection(4)">Continue</div>
    </section>

    <section id="section4">
                <p>\[ C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - f_{\beta}(x^{(i)}))^2 \]</p>
                <p class="math-step-explanation">Let's break this down:</p>
        <div class="continue-button" onclick="showNextSection(5)">Continue</div>
    </section>

    <section id="section5">
       
                <p>\[ n \]</p>
                <p class="math-step-explanation">The total number of data points in our training set.</p>
            <div class="continue-button" onclick="showNextSection(6)">Continue</div>
    </section>

    <section id="section6">
            
                <p>\[ i=1 \dots n \]</p>
                <p class="math-step-explanation">We are summing over all data points, from the first (\(i=1\)) to the last (\(i=n\)).</p>
     <div class="continue-button" onclick="showNextSection(7)">Continue</div>
    </section>

    <section id="section7">
        
                <p>\[ y^{(i)} \]</p>
                <p class="math-step-explanation">The actual, true target value for the \(i\)-th data point.</p>
       
             <div class="continue-button" onclick="showNextSection(8)">Continue</div>
    </section>

    <section id="section8">
                <p>\[ f_{\beta}(x^{(i)}) \]</p>
                <p class="math-step-explanation">Our model's prediction for the \(i\)-th data point, using the current parameters \(\beta\).</p>
         
        <div class="continue-button" onclick="showNextSection(9)">Continue</div>
    </section>

    <section id="section9">
                <p>\[ (y^{(i)} - f_{\beta}(x^{(i)})) \]</p>
                <p class="math-step-explanation">The error (or residual) for the \(i\)-th data point – the difference between the actual value and our prediction.</p>
             <div class="continue-button" onclick="showNextSection(10)">Continue</div>
    </section>

    <section id="section10">
           
                <p>\[ (...)^2 \]</p>
                <p class="math-step-explanation">We square the error. This does two things: makes errors positive (so over- and under-predictions don't cancel out) and penalizes larger errors more heavily.</p>
          <div class="continue-button" onclick="showNextSection(11)">Continue</div>
    </section>

    <section id="section11">
            
                <p>\[ \sum_{i=1}^{n} \]</p>
                <p class="math-step-explanation">We sum up these squared errors across all \(n\) data points.</p>
       
        <div class="continue-button" onclick="showNextSection(12)">Continue</div>
    </section>

    <section id="section12">
                <p>\[ \frac{1}{2n} \]</p>
                <p class="math-step-explanation">We average the total squared error by dividing by \(n\). The extra factor of \(1/2\) is just for mathematical convenience – it makes the derivative cleaner, as we'll see shortly!</p>
         
         <div class="continue-button" onclick="showNextSection(13)">Continue</div>
    </section>

    <section id="section13">
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="A 2D scatter plot with about 10 points. A line (representing f_β(x)) is drawn through them, clearly not perfectly fitting. For one specific point (x^(i), y^(i)), draw a vertical line segment connecting y^(i) to the prediction f_β(x^(i)) on the regression line. Label this segment 'Error: (y^(i) - f_β(x^(i)))'. Maybe lightly shade the squared area corresponding to this error.">
        </div>
         <div class="continue-button" onclick="showNextSection(14)">Continue</div>
    </section>

    <section id="section14">
        <p>Our goal with Gradient Descent is now clear: find the values of \(\beta_0, \beta_1, \dots, \beta_m\) that make this MSE cost \(C(\beta)\) as small as possible.</p>
        
        <div class="continue-button" onclick="showNextSection(15)">Continue</div>
    </section>

    <section id="section15">
        <h2>Calculating the Gradient: Partial Derivatives</h2>
        <p>To use our update rule \(\beta_{new} = \beta_{old} - \alpha \nabla_{\beta} C(\beta_{old})\), we need the gradient \(\nabla_{\beta} C(\beta)\). Remember, the gradient is just a vector containing the partial derivative of the cost \(C\) with respect to <em>each</em> parameter \(\beta_j\).</p>
        
        <p>So, we need to calculate: \(\frac{\partial C}{\partial \beta_0}, \frac{\partial C}{\partial \beta_1}, \dots, \frac{\partial C}{\partial \beta_m}\).</p>
                <div class="continue-button" onclick="showNextSection(16)">Continue</div>
    </section>

    <section id="section16">
        <p>Let's roll up our sleeves and use some calculus (specifically, the chain rule!). We'll find the derivative for a general \(\beta_j\) (where \(j\) could be \(1, 2, \dots, m\)) and then handle the special case for \(\beta_0\).</p>
        
         <div class="continue-button" onclick="showNextSection(17)">Continue</div>
    </section>

    <section id="section17">
                <p><strong>Goal</strong></p>
                <p>\[ \text{Find } \frac{\partial C}{\partial \beta_j} \text{ for } C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - f_{\beta}(x^{(i)}))^2 \]</p>
                <p class="math-step-explanation">We want to see how the cost \(C\) changes if we slightly nudge just one parameter \(\beta_j\), keeping all others fixed.</p>
            <div class="continue-button" onclick="showNextSection(18)">Continue</div>
    </section>

    <section id="section18">
            
                <p><strong>Expand \(f_{\beta}\)</strong></p>
                <p>\[ C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - (\beta_0 + \beta_1 x_1^{(i)} + \dots + \beta_j x_j^{(i)} + \dots + \beta_m x_m^{(i)}))^2 \]</p>
                <p class="math-step-explanation">Substitute the full expression for our linear model's prediction.</p>
                   <div class="continue-button" onclick="showNextSection(19)">Continue</div>
    </section>

    <section id="section19">
                <p><strong>Apply Derivative</strong></p>
                <p>\[ \frac{\partial C}{\partial \beta_j} = \frac{\partial}{\partial \beta_j} \left[ \frac{1}{2n} \sum_{i=1}^{n} ( ... )^2 \right] \]</p>
                <p class="math-step-explanation">We want to differentiate this whole expression with respect to \(\beta_j\).</p>
                  <div class="continue-button" onclick="showNextSection(20)">Continue</div>
    </section>

    <section id="section20">
                <p><strong>Constants & Sum Rule</strong></p>
                <p>\[ = \frac{1}{2n} \sum_{i=1}^{n} \frac{\partial}{\partial \beta_j} \left[ (y^{(i)} - f_{\beta}(x^{(i)}))^2 \right] \]</p>
                <p class="math-step-explanation">The constant \(1/(2n)\) pulls out, and the derivative of a sum is the sum of the derivatives.</p>
                  <div class="continue-button" onclick="showNextSection(21)">Continue</div>
    </section>

    <section id="section21">
                <p><strong>Chain Rule (Outer)</strong></p>
                <p>\[ = \frac{1}{2n} \sum_{i=1}^{n} 2 (y^{(i)} - f_{\beta}(x^{(i)})) \cdot \frac{\partial}{\partial \beta_j} (y^{(i)} - f_{\beta}(x^{(i)})) \]</p>
                <p class="math-step-explanation">Apply the chain rule: derivative of \(u^2\) is \(2u \cdot \frac{du}{d\beta_j}\).</p>
                    <div class="continue-button" onclick="showNextSection(22)">Continue</div>
    </section>

    <section id="section22">
                <p><strong>Chain Rule (Inner)</strong></p>
                <p>\[ \frac{\partial}{\partial \beta_j} (y^{(i)} - (\beta_0 + \dots + \beta_j x_j^{(i)} + \dots)) \]</p>
                <p class="math-step-explanation">Now differentiate the inner part \((y^{(i)} - f_{\beta}(x^{(i)}))\) with respect to \(\beta_j\). \(y^{(i)}\) is a constant w.r.t. \(\beta_j\). Most \(\beta_k x_k^{(i)}\) terms are also constant w.r.t. \(\beta_j\) (when \(k \neq j\)). The only term that matters is \(-\beta_j x_j^{(i)}\).</p>
              <div class="continue-button" onclick="showNextSection(23)">Continue</div>
    </section>

    <section id="section23">
                <p><strong>Inner Derivative Result</strong></p>
                <p>\[ \frac{\partial}{\partial \beta_j} (y^{(i)} - f_{\beta}(x^{(i)})) = -x_j^{(i)} \]</p>
                <p class="math-step-explanation">The derivative of \(-\beta_j x_j^{(i)}\) with respect to \(\beta_j\) is just \(-x_j^{(i)}\) (since \(x_j^{(i)}\) is treated as a constant factor here).</p>
                 <div class="continue-button" onclick="showNextSection(24)">Continue</div>
    </section>

    <section id="section24">
                <p><strong>Combine</strong></p>
                <p>\[ \frac{\partial C}{\partial \beta_j} = \frac{1}{2n} \sum_{i=1}^{n} 2 (y^{(i)} - f_{\beta}(x^{(i)})) \cdot (-x_j^{(i)}) \]</p>
                <p class="math-step-explanation">Substitute the inner derivative back into the equation.</p>
                   <div class="continue-button" onclick="showNextSection(25)">Continue</div>
    </section>

    <section id="section25">
                <p><strong>Simplify</strong></p>
                <p>\[ = \frac{1}{n} \sum_{i=1}^{n} (f_{\beta}(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \]</p>
                <p class="math-step-explanation">Cancel the 2s. Flip the sign inside the parenthesis by absorbing the minus sign from \(-x_j^{(i)}\). This is the final form!</p>
             <div class="continue-button" onclick="showNextSection(26)">Continue</div>
    </section>

    <section id="section26">
        
        <p>Phew! So, the partial derivative of the MSE cost with respect to a parameter \(\beta_j\) (for \(j=1, \dots, m\)) is the average of the error \((f_{\beta}(x^{(i)}) - y^{(i)})\) multiplied by the corresponding feature value \(x_j^{(i)}\) for that data point.</p>
        
        <div class="continue-button" onclick="showNextSection(27)">Continue</div>
    </section>

    <section id="section27">
        <h2>The Special Case: Intercept Term (β₀)</h2>
        <p>What about \(\beta_0\), the intercept term? We can use the same formula we just derived!</p>
                <div class="continue-button" onclick="showNextSection(28)">Continue</div>
    </section>

    <section id="section28">
        <div class="stop-and-think">
            <h3>Stop and Think</h3>
            <h4>How can we use the formula \(\frac{\partial C}{\partial \beta_j} = \frac{1}{n} \sum_{i=1}^{n} (f_{\beta}(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}\) for \(\beta_0\)? What is the 'feature value' \(x_0^{(i)}\) that corresponds to the intercept \(\beta_0\) in our model \(f_{\beta}(x^{(i)}) = \beta_0 + \beta_1 x_1^{(i)} + \dots\)?</h4>
            <button class="reveal-button" onclick="revealAnswer('stop-and-think-1')">Reveal</button>
            <p id="stop-and-think-1" style="display: none;">Think about how \(\beta_0\) appears in the sum: it's like \(\beta_0 \times 1\). So, we can imagine a 'dummy' feature \(x_0\) that always has the value 1 for every data point \(i\). Therefore, \(x_0^{(i)} = 1\).</p>
        </div>
        
        <p>Exactly! By convention, we assume there's an implicit feature \(x_0^{(i)}\) which is always equal to 1 for all data points \(i\). So, we can just plug \(j=0\) and \(x_0^{(i)} = 1\) into our general formula:</p>
        
        <div class="continue-button" onclick="showNextSection(29)">Continue</div>
    </section>

    <section id="section29">
                <p>\[ \frac{\partial C}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} (f_{\beta}(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)} \]</p>
                <p class="math-step-explanation">Using the general formula with \(j=0\).</p>
          <div class="continue-button" onclick="showNextSection(30)">Continue</div>
    </section>

    <section id="section30">
                <p>\[ = \frac{1}{n} \sum_{i=1}^{n} (f_{\beta}(x^{(i)}) - y^{(i)}) \cdot 1 \]</p>
                <p class="math-step-explanation">Substitute \(x_0^{(i)} = 1\).</p>
        <div class="continue-button" onclick="showNextSection(31)">Continue</div>
    </section>

    <section id="section31">
                <p>\[ = \frac{1}{n} \sum_{i=1}^{n} (f_{\beta}(x^{(i)}) - y^{(i)}) \]</p>
                <p class="math-step-explanation">The partial derivative for the intercept is simply the average error across all data points.</p>
        <div class="continue-button" onclick="showNextSection(32)">Continue</div>
    </section>

    <section id="section32">
        
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Partial Derivative (∂C/∂βⱼ)</h4>
            <p>Measures how the cost function \(C\) changes when only the parameter \(\beta_j\) is changed slightly, holding all other parameters constant. This tells us the slope of the cost function along the direction of that specific parameter.</p>
            <h4 class="vocab-term">Intercept (β₀)</h4>
            <p>The parameter in a linear model that represents the predicted value when all features are zero. It corresponds to a conceptual feature \(x_0\) that is always 1.</p>
        </div>
        
        <div class="continue-button" onclick="showNextSection(33)">Continue</div>
    </section>

    <section id="section33">
        <h2>Putting It All Together: The Update Rules</h2>
        <p>Okay, we've done the hard part! We have the general Gradient Descent rule from Lesson 1, and now we have the specific partial derivatives for linear regression with MSE cost.</p>
        <p>Let's write down the complete update steps:</p>
        <div class="continue-button" onclick="showNextSection(34)">Continue</div>
    </section>

    <section id="section34">
        <p>We need to update <strong>all</strong> parameters \(\beta_0, \beta_1, \dots, \beta_m\) <strong>simultaneously</strong> in each iteration. The rules are:</p>
        
             <div class="continue-button" onclick="showNextSection(35)">Continue</div>
    </section>

    <section id="section35">
                <p><strong>Update Intercept</strong></p>
                <p>\[ \beta_0 := \beta_0 - \alpha \frac{1}{n} \sum_{i=1}^{n} (f_{\beta}(x^{(i)}) - y^{(i)}) \]</p>
                <p class="math-step-explanation">Update \(\beta_0\) by subtracting the learning rate times the average error.</p>
         <div class="continue-button" onclick="showNextSection(36)">Continue</div>
    </section>

    <section id="section36">
                <p><strong>Update Slopes (j=1...m)</strong></p>
                <p>\[ \beta_j := \beta_j - \alpha \frac{1}{n} \sum_{i=1}^{n} (f_{\beta}(x^{(i)}) - y^{(i)}) x_j^{(i)} \]</p>
                <p class="math-step-explanation">Update each slope \(\beta_j\) by subtracting the learning rate times the average of (error times corresponding feature value).</p>
        <div class="continue-button" onclick="showNextSection(37)">Continue</div>
    </section>

    <section id="section37">
        
        <p>Notice the summation \(\sum_{i=1}^{n}\) in both update rules. To calculate the gradient components for <em>one</em> update step, we need to compute the prediction error \((f_{\beta}(x^{(i)}) - y^{(i)})\) for <strong>every single data point</strong> in our training set and then sum them up (or average them).</p>
                <div class="continue-button" onclick="showNextSection(38)">Continue</div>
    </section>

    <section id="section38">
        <div class="vocab-section">
            <h3>Build Your Vocab</h3>
            <h4 class="vocab-term">Batch Gradient Descent</h4>
            <p>The version of Gradient Descent where the gradient is calculated using the entire training dataset (the full 'batch') in each iteration. This leads to a smooth convergence path but can be slow for very large datasets.</p>
        </div>
        
        <p>Because we use the entire <strong>batch</strong> of training data (\(n\) samples) to compute the gradient in each step, this specific algorithm is formally called <strong>Batch Gradient Descent</strong>.</p>
                <div class="continue-button" onclick="showNextSection(39)">Continue</div>
    </section>

    <section id="section39">
        <div class="test-your-knowledge">
            <h3>Test Your Knowledge</h3>
            <h4>In Batch Gradient Descent for linear regression, if you have 1 million data points (\(n=1,000,000\)), how many data points do you need to process to calculate the gradient and perform just <em>one</em> update of the parameters \(\beta\)?</h4>
            <div class="options-container">
                <div class="option" onclick="checkAnswer(this, false)">
                    Just 1 data point.
                    <div class="option-explanation">Using just one point is characteristic of Stochastic Gradient Descent (SGD), not Batch GD.</div>
                </div>
                <div class="option" onclick="checkAnswer(this, false)">
                    A small subset, like 100 points.
                    <div class="option-explanation">Using a subset is characteristic of Mini-batch Gradient Descent, not Batch GD.</div>
                </div>
                <div class="option" onclick="checkAnswer(this, true)">
                    All 1 million data points.
                    <div class="option-explanation">Correct! Batch GD requires summing the errors (and error*feature) across the entire dataset (\(n\) points) for each single update step.</div>
                </div>
                <div class="option" onclick="checkAnswer(this, false)">
                    It depends on the number of features, \(m\).
                    <div class="option-explanation">The number of features affects the <em>number</em> of parameters to update, but Batch GD always uses all \(n\) data points to calculate the gradient for each update.</div>
                </div>
            </div>
        </div>
        
        <div class="continue-button" onclick="showNextSection(40)">Continue</div>
    </section>

    <section id="section40">
        <h2>Next Steps</h2>
        <div class="image-placeholder">
            <img src="/placeholder.svg?height=300&width=600" alt="Diagram summarizing the process: Linear Model -> MSE Cost Function -> Calculate Partial Derivatives -> Batch GD Update Rules.">
        </div>
        <p>Fantastic! We've now connected the general idea of Gradient Descent to the specifics of linear regression.</p>
        <p>We defined the MSE cost function and, step-by-step, derived the partial derivatives needed for the gradient. We saw how to handle both the intercept (\(\beta_0\)) and slope (\(\beta_j\)) parameters, leading to the specific update rules for Batch Gradient Descent.</p>
        <p><strong>Key Takeaway:</strong> Batch GD updates parameters based on the average error signal across the <em>entire</em> dataset.</p>
        <div class="continue-button" onclick="showNextSection(41)">Continue</div>
    </section>

    <section id="section41">
        <div class="why-it-matters">
            <h3>Why It Matters</h3>
            <p>Understanding how to derive and apply GD to linear regression is foundational. The same principles – defining a cost function, calculating its gradient, and iteratively updating parameters – apply to optimizing much more complex models in machine learning.</p>
        </div>
        <div class="continue-button" onclick="showNextSection(42)">Continue</div>
    </section>

    <section id="section42">
        <p>Now that we know <em>how</em> to calculate the steps for Batch GD, we need to consider the bigger picture. How does the full algorithm run? When does it stop? And what happens if the cost function isn't a nice simple bowl shape? We'll explore the full algorithm structure and the challenge of local minima in the next lesson.</p>
    </section>

    <script>
        // Show the first section initially
        document.getElementById("section1").style.display = "block";
        document.getElementById("section1").style.opacity = "1";

        function showNextSection(nextSectionId) {
            const currentButton = event.target;
            const nextSection = document.getElementById("section" + nextSectionId);
            
            currentButton.style.display = "none";
            
            nextSection.style.display = "block";
            setTimeout(() => {
                nextSection.style.opacity = "1";
            }, 10);

            setTimeout(() => {
                nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 500);
        }

        function revealAnswer(id) {
            const revealText = document.getElementById(id);
            const revealButton = event.target;
            
            revealText.style.display = "block";
            revealButton.style.display = "none";
        }
        
        function checkAnswer(element, isCorrect) {
            // Remove any previous classes
            const options = document.querySelectorAll('.option');
            options.forEach(option => {
                option.classList.remove('correct', 'incorrect');
            });
            
            // Add appropriate class
            if (isCorrect) {
                element.classList.add('correct');
            } else {
                element.classList.add('incorrect');
            }
            
            // Show explanation
            const explanation = element.querySelector('.option-explanation');
            explanation.style.display = 'block';
        }
    </script>
</body>
</html>
